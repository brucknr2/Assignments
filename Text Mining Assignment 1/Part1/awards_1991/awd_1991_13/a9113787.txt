Title       : A Gesture Interpretation And Voice Recognition Multi-Modal Human Machine
               Interface
Type        : Award
NSF Org     : IIS 
Latest
Amendment
Date        : August 23,  1993    
File        : a9113787

Award Number: 9113787
Award Instr.: Continuing grant                             
Prgm Manager: Oscar N. Garcia                         
	      IIS  DIV OF INFORMATION & INTELLIGENT SYSTEMS
	      CSE  DIRECT FOR COMPUTER & INFO SCIE & ENGINR
Start Date  : July 1,  1992       
Expires     : June 30,  1994       (Estimated)
Expected
Total Amt.  : $151071             (Estimated)
Investigator: Francis Quek   (Principal Investigator current)
              Ramesh C. Jain  (Co-Principal Investigator former)
Sponsor     : University of Michigan
	      3003 S State St. RM 1062
	      Ann Arbor, MI  481091274    313/764-1817

NSF Program : 6845      HUMAN COMPUTER INTER PROGRAM
Fld Applictn: 0104000   Information Systems                     
              99        Other Sciences NEC                      
Program Ref : 9146,9216,HPCC,MANU,
Abstract    :
                9113787  Quek    This the third year of a three year continuing grant.  This 
              research is to study computer vision methods for tracking hand  gestures. 
              Gestures are followed by tracking a light emitting  diodes (LED) adorned glove
              in front of a video system.  Other glove  marking methods will also be
              examined.  Gesture interpretation will  be performed by a dynamic vision system
              using the correspondence  via event detection (CED) algorithm which segments
              and tracks the  motion of the LEDs over multiple video frames.  The primary
              focus  of the research will be to extend existing algorithms to handle 
              occlusion.  Constraints inherent in hand anatomy, function and  gesture
              categories will be expoited.  Voice input will be  implemented using commercial
              disconnected speech recognition  technology.  The voice commands will be used
              segment gesture epochs  and to disambiguate the gesture categories.  Rules for
              combining  gesture interpretation and voice for gesture stream segmentation 
              will be developed.  A gesture library constituting a taxonomy of  gestures will
              be used for temporal coordination and conflict  resolution.  Both monocular and
              stereo camera environments will be  used.  Parallelization of the algorithm
              will be done for to assure  real-time capability and to support subsequent
              research where bare  hands might be tracked without the use of markers.
