Title       : Post Doctoral: Probabilistic Models for Hierarchical Neural Networks
Type        : Award
NSF Org     : EIA 
Latest
Amendment
Date        : May 31,  1994       
File        : a9404932

Award Number: 9404932
Award Instr.: Standard Grant                               
Prgm Manager: Tse-yun Feng                            
	      EIA  DIVISION OF EXPERIMENTAL & INTEG ACTIVIT
	      CSE  DIRECT FOR COMPUTER & INFO SCIE & ENGINR
Start Date  : September 1,  1994  
Expires     : October 31,  1996    (Estimated)
Expected
Total Amt.  : $43488              (Estimated)
Investigator: Michael I. Jordan   (Principal Investigator current)
Sponsor     : MIT
	      77 Massachusetts Avenue
	      Cambridge, MA  021394307    617/253-1000

NSF Program : 2885      CISE RESEARCH INFRASTRUCTURE
Fld Applictn: 0000099   Other Applications NEC                  
              31        Computer Science & Engineering          
Program Ref : 9178,9192,SMET,
Abstract    :
              9404932  Jordan     The Associateship in Experimental Science will support a
              program of interdisciplinary research at the Massachusetts Institute of
              Technology.  The proposed research will focus on probabilistic models for
              hierarchical neural networks.  Two specific architectures will be studied in
              detail: hierarchical mixtures-of-experts (HME) and Boltzmann trees.     HME
              networks that model mixtures of more complicated probabilistic processes, such
              as hidden Markov chains and optimal observers will be developed.  The
              expectation-maximization (EM) principle from statistics will be utilized to
              develop learning algorithms for these architectures.  The behavior of these EM
              algorithms will be analyzed by exploiting the relationship of EM to mean-field
              theories from statistical physics.     Boltzmann machines are a general class
              of probabilistic model for constraint satisfaction whose development has been
              hindered by the lack of an efficient learning algorithm.  Boltzmann machines
              with tree-like architectures have simplifying features that make them faster
              and more efficient.  Further work on Boltzmann trees will lead to more powerful
              algorithms for supervised and unsupervised learning.  Various parallels between
              Boltzmann trees and belief networks for probabilistic reasoning will be
              explained.  ***
