Title       : Mathematical Sciences: RUI: Applications of Wavelet Analysis to Neural Networks
Type        : Award
NSF Org     : DMS 
Latest
Amendment
Date        : July 5,  1994       
File        : a9404513

Award Number: 9404513
Award Instr.: Standard Grant                               
Prgm Manager: Michael H. Steuerwalt                   
	      DMS  DIVISION OF MATHEMATICAL SCIENCES       
	      MPS  DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN
Start Date  : October 1,  1994    
Expires     : March 31,  1998      (Estimated)
Expected
Total Amt.  : $60000              (Estimated)
Investigator: Hrushikesh N. Mhaskar   (Principal Investigator current)
Sponsor     : Cal State LA Univ Aux Serv
	      5151 State University Drive
	      Los Angeles, CA  900324221    213/224-3481

NSF Program : 1271      COMPUTATIONAL MATHEMATICS
Fld Applictn: 0000099   Other Applications NEC                  
              21        Mathematics                             
Program Ref : 9178,9229,SMET,
Abstract    :
              The investigator studies the approximation capabilities of  afeedforward neural
              network for approximating functions in  different function classes in terms of
              the activation function  used, the number of layers, and the number of neurons.
                Particular emphasis is placed on dimension-independent bounds and  localized
              approximation by networks.  In particular, the research  establishes strong
              connections between the theory of wavelets and  neural networks.  Applications
              to the construction of universal  time series predictors and pattern
              classifiers also are studied.  The project connects the theory of wavelets with
              that of neural  networks.  Both of these theories have numerous applications in
               such areas as data compression, prediction of time series, target 
              classification, pattern recognition, and signal processing.  Both  topics of
              study have therefore flourished during the recent past,  for the most part,
              independently of each other.  There do exist  certain similarities between the
              two theories stemming from the  fact that they both deal with mathematically
              similar processes of  approximation.  The investigator explores these
              similarities  further.  In particular, he studies the inherent capabilities and
               limitations of universal mapping networks and develops efficient  training
              paradigms.  A remarkable feature of the networks to be  studied is the ease
              with which the same network can be trained  and retrained to perform a variety
              of tasks using a theoretically  guaranteed minimal number of neurons.
