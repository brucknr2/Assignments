Title       : Mathematical Sciences: Investigations Into Computationally Intensive Statistical
               Methods
Type        : Award
NSF Org     : DMS 
Latest
Amendment
Date        : July 7,  1994       
File        : a9404594

Award Number: 9404594
Award Instr.: Standard Grant                               
Prgm Manager: James E. Gentle                         
	      DMS  DIVISION OF MATHEMATICAL SCIENCES       
	      MPS  DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN
Start Date  : July 1,  1994       
Expires     : November 30,  1997   (Estimated)
Expected
Total Amt.  : $60000              (Estimated)
Investigator: Art B. Owen owen@stat.stanford.edu  (Principal Investigator current)
Sponsor     : Stanford University
	      651 Serra St.
	      Stanford, CA  94305    415/723-2300

NSF Program : 1269      STATISTICS
Fld Applictn: 0000099   Other Applications NEC                  
              21        Mathematics                             
Program Ref : 0000,OTHR,
Abstract    :
               9404594   Owen   This project considers modern computationally intensive
              statistical methods, focussing on problems of numerical quadrature in high
              dimensions and neural networks in noisy settings. The work on quadrature will
              develop hybrids of equidistribution methods and Monte Carlo methods, in order
              to combine the best features of each. Equidistribution methods commonly provide
              more accurate estimates of integrals, as borne out by asymptotic calculations
              and some examples in the computational physics literature. Monte Carlo methods,
              make it easier to assess the accuracy of an estimated integral. The hybrid is
              formed by randomizing within a class of equidistribution methods developed by
              Faure and Niederreiter. It is expected that the resulting methods will produce
              accurate answers whose accuracy can be reliably gauged from the same data used
              to generate them. Artificial neural networks are widely used to predict and
              classify responses based on a set of predictors. They are better able to
              estimate complicated structures than many traditional statistical tools. They
              are also more prone to finding structures when given purely random data to
              train on. The problems considered here are guaging how much structure a neural
              network will learn in a noisy setting, and constructing networks that find less
              structure in the noise while remaining sensitive to true structure.  The
              integrals considered here may be thought of as averages of one "output"
              quantity as perhaps ten or twenty "input" quantities vary over their possible
              values. These averages are of interest in problems from chemistry, physics,
              finance and statistics. One approach to calculating these averages is based on
              picking a list of representative input settings, evenly spread through the
              possible input values, and then averaging the corresponding output values. For
              many problems this method is quite accurate, but on any given problem it can be
              hard to tell exactly how accurate the answer is. A second approach uses a
              randomly chosen list of in put settings. This approach is usually less accurate
              but there are ways of using the randomness to make probabilistic accuracy
              statements about the answer. The proposed research combines these ideas by
              taking a representative list of input settings and randomly scrambling it in a
              way that preserves the representativeness but should still allow probabilistic
              statements of accuracy to be made. Artificial neural networks are often used in
              statistical problems such as predicting what group an object belongs to, given
              some measured features of it, or predicting an output number given some input
              numbers. They are called neural networks based on an analogy between their
              structure and that of a brain. They are usually trained on a set of data
              containing the true inputs and outputs and in many problems are effective at
              learning to predict future outputs from future inputs, even when the
              input-output relationship is very complicated. The proposed work is to study
              the extent to which artificial neural networks mistakenly learn random patterns
              from data in which the inputs are irrelevant to the outputs, and to identify
              which sorts of neural networks are less prone to this problem.
