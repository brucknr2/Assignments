Title       : Fast Algorithms for Large Scale Convex Optimization Involving Linear Matrix
               Inequality
Type        : Award
NSF Org     : ECS 
Latest
Amendment
Date        : September 15,  1994 
File        : a9411664

Award Number: 9411664
Award Instr.: Standard Grant                               
Prgm Manager: Radhakisan S. Baheti                    
	      ECS  DIV OF ELECTRICAL AND COMMUNICATIONS SYS
	      ENG  DIRECTORATE FOR ENGINEERING             
Start Date  : September 15,  1994 
Expires     : August 31,  1998     (Estimated)
Expected
Total Amt.  : $163215             (Estimated)
Investigator: Ko-Hui Michael Fan fan@ee.gatech.edu  (Principal Investigator current)
Sponsor     : GA Tech Res Corp - GIT
	      Office of Sponsored Programs
	      Atlanta, GA  303320420    404/385-0866

NSF Program : 1518      CONTROL, NETWORKS, & COMP INTE
Fld Applictn: 0112000   System Theory                           
              55        Engineering-Electrical                  
Program Ref : 0000,9148,OTHR,
Abstract    :
              9411664  Fan  With recent dramatic increase in available computing power, 
              numerical optimization has become an attractive tool for analysis  and design
              of complex systems.  There are a large number of  problems in science and
              engineering which can be formulated as  convex optimization problems involving
              linear matrix inequality.   This problem is inherently difficult to solve as
              the  nonsmoothness usually occurs at the solution.  Further, as  systems are
              becoming more complex, the resulting optimization  problems tend to have a
              large number of decision variables as  well as constraints.  Therefore, several
              basic computation  components such as inversion of a matrix, eigenvalue and 
              eigenvector calculation, or evaluation of the Hessian matrix,  that needed for
              most methods become either prohibited or very  costly to perform.  It is then
              necessary to use approximations  and yet have the fast rate of convergence
              preserved.  ***
