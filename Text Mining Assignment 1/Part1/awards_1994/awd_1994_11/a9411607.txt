Title       : Visual Speech Articulation Training Aid
Type        : Award
NSF Org     : BES 
Latest
Amendment
Date        : June 26,  1996      
File        : a9411607

Award Number: 9411607
Award Instr.: Continuing grant                             
Prgm Manager: Gilbert B. Devey                        
	      BES  DIV OF BIOENGINEERING & ENVIRON SYSTEMS 
	      ENG  DIRECTORATE FOR ENGINEERING             
Start Date  : September 1,  1994  
Expires     : August 31,  1998     (Estimated)
Expected
Total Amt.  : $188006             (Estimated)
Investigator: Stephen A. Zahorian   (Principal Investigator current)
Sponsor     : Old Dominion Research Fdn
	      800 W. 46th Street
	      Norfolk, VA  23508    757/683-4293

NSF Program : 5342      RESEARCH TO AID THE DISABLED
Fld Applictn: 0203000   Health                                  
              59        Engineering NEC                         
Program Ref : 9181,BIOT,
Abstract    :
              9411607  Zahorian  The research proposed in this project is to investigate and 
              optimize methods for extracting speaker-independent acoustically-  based speech
              parameters that signal the phonetic content of  speech and to determine
              transformations which can display these  features in an articulation training
              aid for the hearing  impaired.  The proposed project is a continuation and
              extension  of research previously funded by the NSF.  A major component of  the
              effort will be to coordinate the collection of a large data  base of speech
              samples from both normally-hearing and hearing-  impaired listeners, which will
              be required not only for the  proposed work, but which will also facilitate
              other research  efforts in the field.  Extensive experiments will be conducted
              to  optimize the conversion of speech parameters to display  parameters so that
              phonemes and distinctive features of phonemes,  produced either in isolation or
              syllabic contexts, can readily be  discriminated and identified based on their
              display  characteristics.  A combination  of nonlinear/linear transform  will
              convert acoustic measurements of auditory stimuli to a  visual display
              representation.  Acoustic features will be  extracted from global spectral
              shape, fundamental frequency, and  short-time energy.  The feature
              extraction/classification process  will be individually optimized for every
              pair of phones in  English.  ***
