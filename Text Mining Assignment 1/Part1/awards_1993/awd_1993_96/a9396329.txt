Title       : A Gesture Interpretation And Voice Recognition Multi-Modal Human Machine
               Interface
Type        : Award
NSF Org     : IIS 
Latest
Amendment
Date        : June 10,  1996      
File        : a9396329

Award Number: 9396329
Award Instr.: Continuing grant                             
Prgm Manager: Gary W Strong                           
	      IIS  DIV OF INFORMATION & INTELLIGENT SYSTEMS
	      CSE  DIRECT FOR COMPUTER & INFO SCIE & ENGINR
Start Date  : June 1,  1993       
Expires     : August 31,  1996     (Estimated)
Expected
Total Amt.  : $436870             (Estimated)
Investigator: Francis Quek   (Principal Investigator current)
              Ramesh C. Jain  (Co-Principal Investigator current)
Sponsor     : U of Illinois Chicago
	      1737 W. Polk Street
	      Chicago, IL  60612    312/996-7000

NSF Program : 6845      HUMAN COMPUTER INTER PROGRAM
Fld Applictn: 
Program Ref : 
Abstract    :
                9113787  Quek    This the third year of a three year continuing grant.  This 
              research is to study computer vision methods for tracking hand  gestures. 
              Gestures are followed by tracking a light emitting  diodes (LED) adorned glove
              in front of a video system.  Other glove  marking methods will also be
              examined.  Gesture interpretation will  be performed by a dynamic vision system
              using the correspondence  via event detection (CED) algorithm which segments
              and tracks the  motion of the LEDs over multiple video frames.  The primary
              focus  of the research will be to extend existing algorithms to handle 
              occlusion.  Constraints inherent in hand anatomy, function and  gesture
              categories will be expoited.  Voice input will be  implemented using commercial
              disconnected speech recognition  technology.  The voice commands will be used
              segment gesture epochs  and to disambiguate the gesture categories.  Rules for
              combining  gesture interpretation and voice for gesture stream segmentation 
              will be developed.  A gesture library constituting a taxonomy of  gestures will
              be used for temporal coordination and conflict  resolution.  Both monocular and
              stereo camera environments will be  used.  Parallelization of the algorithm
              will be done for to assure  real-time capability and to support subsequent
              research where bare  hands might be tracked without the use of markers.
