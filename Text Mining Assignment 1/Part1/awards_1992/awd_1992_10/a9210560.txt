Title       : Instruction of a Robot Using Visual Commands
Type        : Award
NSF Org     : IIS 
Latest
Amendment
Date        : June 28,  1994      
File        : a9210560

Award Number: 9210560
Award Instr.: Continuing grant                             
Prgm Manager: Howard Moraff                           
	      IIS  DIV OF INFORMATION & INTELLIGENT SYSTEMS
	      CSE  DIRECT FOR COMPUTER & INFO SCIE & ENGINR
Start Date  : July 1,  1992       
Expires     : December 31,  1995   (Estimated)
Expected
Total Amt.  : $96254              (Estimated)
Investigator: Jill D. Crisman crisman@ece.neu.edu  (Principal Investigator current)
Sponsor     : Northeastern University
	      360 Huntington Avenue
	      Boston, MA  021155096    617/373-5600

NSF Program : 6840      ROBOTICS AND HUMAN AUGMENTATIO
Fld Applictn: 0104000   Information Systems                     
              99        Other Sciences NEC                      
Program Ref : 9146,9264,MANU,
Abstract    :
                 Discovering the fundamental vision-motion primitives that are               
              used by people to navigate and manipulate objects would lead to a              
              natural human-machine interface for programming robotic systems                
              especially in unstructured environments.  In this proposal, we                 
              begin exploring the definition of a set of natural robotic                     
              interface commands used for navigation.  A more natural                        
              navigational interface would allow robots to be more easily                    
              integrated into applications such as material handling for                     
              flexible manufacturing, planetary or underwater exploration, or                
              automated wheelchairs.  The programmer issues a command such as                
              "go there" to the robot by specifying "there" as a location on a               
              video screen.  The robot then navigates to the desired location                
              on a video screen.  The robot then navigates to the desired                    
              location, avoiding any obstacles along the way.  As a tool for                 
              discovering a complete set of navigational commands, we will                   
              implement and experiment with an initial set of commands.  This                
              experimentation should show where our initial command set is                   
              redundant or lacking and is essential for insuring that the                    
              commands are natural for the user.  As part of this research, we               
              will be developing a vision algorithm that can extract visually                
              distinctive features from a wide variety of objects.  The command              
              set and the algorithms for feature extraction and tracking will                
              be the largest contribution of this work.  Future work involves                
              further testing of the navigational commands on a convenient test              
              platform and extending the methodology derived for discovering                 
              the primitive navigation commands to deriving manipulation                     
              commands.//
