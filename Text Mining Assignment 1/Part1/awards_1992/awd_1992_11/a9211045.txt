Title       : Explanation-Based Learning: Finding Better Explanations Via Partial Evaluation
Type        : Award
NSF Org     : IIS 
Latest
Amendment
Date        : August 9,  1993     
File        : a9211045

Award Number: 9211045
Award Instr.: Continuing grant                             
Prgm Manager: Larry H. Reeker                         
	      IIS  DIV OF INFORMATION & INTELLIGENT SYSTEMS
	      CSE  DIRECT FOR COMPUTER & INFO SCIE & ENGINR
Start Date  : July 1,  1992       
Expires     : December 31,  1994   (Estimated)
Expected
Total Amt.  : $60000              (Estimated)
Investigator: Oren Etzioni etzioni@cs.washington.edu  (Principal Investigator current)
Sponsor     : U of Washington
	      3935 University Way NE
	      Seattle, WA  981056613    206/543-4043

NSF Program : 6856      KNOWLEDGE & COGNITIVE SYSTEMS
Fld Applictn: 0104000   Information Systems                     
              99        Other Sciences NEC                      
Program Ref : 9218,9264,
Abstract    :
              Controlling search is a central concern for AI.  Overcoming                    
              combinatorial search in realistic planning, design, and reasoning              
              problems requires large doses of domain specific search control                
              knowledge.  Explanation Based Learning (EBL) has emerged as a                  
              standard technique for acquiring search control knowledge.                     
              Previous EBL work has produced impressive demonstrations but has               
              also uncovered a fundamental problem EBL frequently constructs                 
              overlycomplex explanations that yield ineffective control                      
              knowledge.  This research describes a solution:  integrating EBL               
              with partial evaluation to improve EBL's explanations.  In                     
              standard EBL systems, the problem solver's behavior on a training              
              example determines what EBL explains and how.  Partial                         
              evaluation, in contrast, performs a global analysis that often                 
              yields simpler and more general explanations.  In previous work,               
              STATIC (a partial evaluator written by the PI) was pitted against              
              PRODIGY/EBL, a state of the art EBL system.  When tested in                    
              PRODIGY/EBL's benchmark problem spaces, STATIC generated search                
              control knowledge that was up to three times a effective as                    
              PRODIGY/EBL's, and did so twenty six to seventy seven times                    
              faster.  Since STATIC's analysis in not focused by training                    
              examples, however, it may flounder when confronted with large and              
              complex problem spaces.   The PI intends to design and build a                 
              hybrid system , called DYNAMIC, that will overcome the weaknesses              
              of both approaches.  DYNAMIC will identify learning opportunities              
              a la PRODIGY/EBL, BUT GENERATE EXPLANATIONS a la STATIC.  The                  
              detailed studies of the two systems suggest that DYNAMIC will                  
              significantly out perform both, and yield insights in two                      
              fundamental questions: how to improve machine generated                        
              explanations, and what is the appropriate role of training                     
              examples in explanation based learning?                                        
               //
