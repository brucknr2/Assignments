Title       : Audio-Visual Kinematic Specification of Articulatory Dynamics
Type        : Award
NSF Org     : BCS 
Latest
Amendment
Date        : May 17,  1995       
File        : a9212225

Award Number: 9212225
Award Instr.: Continuing grant                             
Prgm Manager: Paul G. Chapin                          
	      BCS  DIVISION OF BEHAVIORAL AND COGNITIVE SCI
	      SBE  DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE
Start Date  : September 15,  1992 
Expires     : August 31,  1996     (Estimated)
Expected
Total Amt.  : $150513             (Estimated)
Investigator: Lawrence Rosenblum rosenblu@citrus.ucr.edu  (Principal Investigator current)
Sponsor     : U of Cal Riverside
	      Office of Research Affairs
	      Riverside, CA  925210217    909/787-5535

NSF Program : 1311      LINGUISTICS
Fld Applictn: 0000099   Other Applications NEC                  
              84        Linguistics                             
Program Ref : 0000,1311,OTHR,
Abstract    :
                                                                                              
                                                                                             
                                                                                             
                                         ABSTRACT                                            
                                                                                             
                  Recently, it has become apparent that a thorough                          
              understanding of speech perception must involve an understanding               
              of how audio-visual speech is perceived (Summerfield, 1987).                   
              However, relatively little work has been conducted to determine                
              the informational metric for audio-visual speech integration.                  
              This research has been designed to determine what this metric is               
              and whether the form of the information for auditory and visual                
              perception is similar.                                                         
                   The first series of experiments tests if the perceptual                   
              primitives of audio-visual speech perception are articulatory or               
              low-level energy properties.  Computer-animated visual displays                
              coupled with discrepant auditory information will be used.                     
              The second series of experiments tests the feasibility of                      
              modality-independent speech information (Summerfield, 1987).  A                
              visual display technique which allows for efficient kinematic                  
              analyses will be implemented (Johansson, 1974).                                
                   The final series of experiments have been designed to                     
              determine if kinematic information for speech can specify                      
              articulatory dynamics.  Visual kinematic properties which are                  
              specified to dynamic articulatory states will be determined and                
              then tested in perceptual experiments.                                         
                   These experiments should add to our understanding of                      
              speech perception and, specifically, speech (lip) reading.  The                
              proposed kinematic analyses and computer animation techniques                  
              should help determine the salient kinematic parameters which                   
              underlie speech reading.
